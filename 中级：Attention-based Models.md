## Attention-based Models (注意力机制模型, 扩展 Transformer)
- 重要性：
Transformer 的注意力机制是现代深度学习的基石，衍生出如 BERT、GPT 等模型，驱动了 NLP 和多模态任务。  
高级班可以深入探讨注意力机制的变种（如多头注意力、自我注意力）。  
- 核心概念：
注意力机制让模型聚焦输入中最重要的部分（如句子中的关键词），通过“查询-键-值”机制计算权重。  
- 应用：聊天机器人（如 Grok）、机器翻译、文本摘要。
<img width="998" height="641" alt="image" src="https://github.com/user-attachments/assets/a78ff1d6-3d30-43e6-b8e2-40acad211a7f" /> 
深度学习中的注意力机制（Attention Mechanism）是一种模仿人类视觉和认知系统的方法，它允许神经网络在处理输入数据时集中注意力于相关的部分。通过引入注意力机制，神经网络能够自动地学习并选择性地关注输入中的重要信息，提高模型的性能和泛化能力。  
上面这张图可以较好地去理解注意力机制，其展示了人类在看到一幅图像时如何高效分配有限注意力资源的，其中红色区域表明视觉系统更加关注的目标，从图中可以看出：人们会把注意力更多的投入到人的脸部
