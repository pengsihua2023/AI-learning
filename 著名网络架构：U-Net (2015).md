## U-Net (2015) 
提出者：Olaf Ronneberger 等  
特点：对称编码-解码结构，跳跃连接保留细节信息，专为图像分割设计。  
应用：医学图像分割、语义分割。  
掌握要点：编码-解码架构、特征融合。  
<img width="1000" height="500" alt="image" src="https://github.com/user-attachments/assets/00b53895-4271-48df-abc5-f39e671ec419" />




## 代码

```
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import os
from tqdm import tqdm

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

# Define data directory (manually set after downloading BraTS 2020)
data_dir = "F:/00-数据/Adam/brats2020"  # Adjust this path to your downloaded data
if not os.path.exists(data_dir):
    raise Exception(f"Directory {data_dir} does not exist. Please download the BraTS 2020 dataset from https://www.med.upenn.edu/cbica/brats2020/data.html, extract it, and place it in {data_dir}.")

# U-Net Model
class UNet(nn.Module):
    def __init__(self, in_channels=4, out_channels=1, init_features=32):
        super(UNet, self).__init__()
        features = init_features
        self.enc1 = self.conv_block(in_channels, features)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.enc2 = self.conv_block(features, features * 2)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.enc3 = self.conv_block(features * 2, features * 4)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        self.bottleneck = self.conv_block(features * 4, features * 8)
        
        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)
        self.dec3 = self.conv_block(features * 8, features * 4)
        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)
        self.dec2 = self.conv_block(features * 4, features * 2)
        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)
        self.dec1 = self.conv_block(features * 2, features)
        
        self.conv_final = nn.Conv2d(features, out_channels, kernel_size=1)
        
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool1(enc1))
        enc3 = self.enc3(self.pool2(enc2))
        
        bottleneck = self.bottleneck(self.pool3(enc3))
        
        dec3 = self.upconv3(bottleneck)
        dec3 = self.crop_and_concat(dec3, enc3)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = self.crop_and_concat(dec2, enc2)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = self.crop_and_concat(dec1, enc1)
        dec1 = self.dec1(dec1)
        
        return self.conv_final(dec1), enc1, enc2, enc3, bottleneck, dec3, dec2, dec1
    
    def crop_and_concat(self, upsampled, bypass):
        diff_y = bypass.size()[2] - upsampled.size()[2]
        diff_x = bypass.size()[3] - upsampled.size()[3]
        if diff_y != 0 or diff_x != 0:
            bypass = bypass[:, :, diff_y//2:-(diff_y//2), diff_x//2:-(diff_x//2)]
        return torch.cat((upsampled, bypass), dim=1)

# Dataset
class Brats2020Dataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.image_paths = []
        self.mask_paths = []
        
        modalities = ['t1', 't2', 'flair', 't1ce']
        for case in os.listdir(data_dir):
            case_path = os.path.join(data_dir, case)
            if os.path.isdir(case_path):
                seg_path = os.path.join(case_path, f"{case}_seg.nii.gz")
                if os.path.exists(seg_path):
                    image_paths = [os.path.join(case_path, f"{case}_{mod}.nii.gz") for mod in modalities]
                    if all(os.path.exists(p) for p in image_paths):
                        self.image_paths.append(image_paths)
                        self.mask_paths.append(seg_path)
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        images = [nib.load(path).get_fdata() for path in self.image_paths[idx]]
        image = np.stack(images, axis=0)  # [4, H, W, D]
        mask = nib.load(self.mask_paths[idx]).get_fdata()  # [H, W, D]
        
        # Take a 2D slice (e.g., middle slice)
        slice_idx = image.shape[2] // 2
        image = image[:, :, slice_idx, :]  # [4, H, W]
        mask = mask[:, :, slice_idx]  # [H, W]
        
        # Normalize image
        image = image.astype(np.float32) / np.max(image)
        mask = (mask > 0).astype(np.float32)  # Binary mask (simplified, can be multi-class)
        
        if self.transform:
            image = np.transpose(image, (1, 2, 0))  # [H, W, 4] to [H, W, C]
            image = self.transform(image)
            mask = self.transform(mask.unsqueeze(0)).squeeze(0)  # Add channel dim for transform
        
        return image, mask

# Visualization
def visualize_features(input_img, output, enc1, enc2, enc3, bottleneck, dec3, dec2, dec1):
    input_img = input_img.cpu().numpy().squeeze().transpose(1, 2, 0)
    output = output.cpu().numpy().squeeze()
    
    if input_img.max() > 1.0:
        input_img = input_img / input_img.max()
    
    enc1 = enc1.cpu().numpy().squeeze()[0]
    enc2 = enc2.cpu().numpy().squeeze()[0]
    enc3 = enc3.cpu().numpy().squeeze()[0]
    bottleneck = bottleneck.cpu().numpy().squeeze()[0]
    dec3 = dec3.cpu().numpy().squeeze()[0]
    dec2 = dec2.cpu().numpy().squeeze()[0]
    dec1 = dec1.cpu().numpy().squeeze()[0]
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes[0, 0].imshow(input_img[:, :, 0], cmap='gray')
    axes[0, 0].set_title("Input Image (T1)")
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(output, cmap='gray')
    axes[0, 1].set_title("Output Segmentation")
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(enc1, cmap='viridis')
    axes[0, 2].set_title("Encoder 1 (First Channel)")
    axes[0, 2].axis('off')
    
    axes[0, 3].imshow(enc2, cmap='viridis')
    axes[0, 3].set_title("Encoder 2 (First Channel)")
    axes[0, 3].axis('off')
    
    axes[1, 0].imshow(enc3, cmap='viridis')
    axes[1, 0].set_title("Encoder 3 (First Channel)")
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(bottleneck, cmap='viridis')
    axes[1, 1].set_title("Bottleneck (First Channel)")
    axes[1, 1].axis('off')
    
    axes[1, 2].imshow(dec3, cmap='viridis')
    axes[1, 2].set_title("Decoder 3 (First Channel)")
    axes[1, 2].axis('off')
    
    axes[1, 3].imshow(dec2, cmap='viridis')
    axes[1, 3].set_title("Decoder 2 (First Channel)")
    axes[1, 3].axis('off')
    
    plt.tight_layout()
    plt.show()

# Main
if __name__ == "__main__":
    # Data
    dataset_path = data_dir
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((512, 512), antialias=True)
    ])
    dataset = Brats2020Dataset(dataset_path, transform=transform)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)
    
    # Model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = UNet(in_channels=4, out_channels=1).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Training
    model.train()
    num_epochs = 10
    for epoch in range(num_epochs):
        running_loss = 0.0
        for batch_idx, (images, masks) in enumerate(tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            images, masks = images.to(device), masks.to(device)
            
            optimizer.zero_grad()
            outputs, enc1, enc2, enc3, bottleneck, dec3, dec2, dec1 = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if batch_idx % 10 == 9:
                print(f"Batch {batch_idx+1}, Loss: {running_loss/10:.4f}")
                running_loss = 0.0
        
        # Visualize a sample
        with torch.no_grad():
            model.eval()
            sample_img, sample_mask = images[0].unsqueeze(0), masks[0].unsqueeze(0)
            output, enc1_v, enc2_v, enc3_v, bottleneck_v, dec3_v, dec2_v, dec1_v = model(sample_img)
            visualize_features(sample_img, output, enc1_v, enc2_v, enc3_v, bottleneck_v, dec3_v, dec2_v, dec1_v)
        model.train()
    
    print("Training finished!")

```

